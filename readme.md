# TENET: Two-Photon Microscopy Image Enhancement NETwork

**TENET** is a fully unsupervised deep-learning framework for simultaneous deblurring, super-resolution (up to 12×), and semantic segmentation of two-photon microscopy (TPM) volumes in a single pass.

> Morita H\*, Hayashi S, Tsuji T, Kato D, Wake H, Shimamura T. "Unsupervised deep learning enables blur-free resolution-enhancement in two-photon microscopy." *(Submitted to Cell Reports Methods)*

---

## Overview

Two-photon microscopy enables non-invasive deep-tissue imaging, but suffers from severe axial blur and anisotropic resolution that hampers 3D quantitative analysis. TENET addresses this by inverting the image formation process:

- A **3D U-Net encoder** (`JNet`) predicts the underlying sharp biological structure (segmentation) and brightness distribution from a blurred input volume.
- A **physics-informed decoder** regenerates the observed blurred image via a differentiable forward model — neural implicit PSF convolution, downsampling, and noise addition.
- A **trainable neural implicit PSF** (`NeurIPSF`) adapts to imaging conditions without requiring rigorous experimental PSF measurement or structural isotropy assumptions.

This self-supervised reconstruction loop requires only raw TPM volumes — no paired sharp ground-truth images are needed.

---

## Key Features

- **Fully unsupervised**: trains on raw TPM volumes without paired labels or measured PSFs
- **Physics-informed**: differentiable blur-generation model with a learnable neural implicit PSF
- **No isotropy assumption**: handles biologically anisotropic structures where CARE and Neuroclear fail
- **All-in-one pipeline**: deblurring + up-to-12× axial super-resolution + semantic segmentation in one forward pass
- **Validated** on synthetic phantoms, fluorescent beads, and in vivo microglia; outperforms RLTV, CARE, and Neuroclear

---


## Getting Started

### Requirements

- Python 3.8+
- PyTorch with CUDA (experiments run on NVIDIA RTX 3090)
- `timm`

```bash
pip install -r requirements.txt
```

### Installation

```bash
git clone https://github.com/HaLU-9000/TENET.git
cd TENET
pip install -r requirements.txt
```

---

## Configuration

Every training and inference run is controlled by a single **JSON config file** in `experiments/configs/`. Scripts take a `model_name` argument which resolves to `experiments/configs/<model_name>.json`. The actual `JNet_beads.json` shipped with the repo is the reference for all field names and types below.

### Config fields quick reference

| Section | Key | Description |
|---|---|---|
| *(root)* | `pretrained_model` | Stem of pre-trained `.pt` checkpoint in `model/` loaded before fine-tuning |
| `params` | `hidden_channels_list` | U-Net channel widths per encoder level |
| | `scale` | Axial super-resolution factor (e.g. `10` = 10×) |
| | `blur_mode` | PSF model: `"gibsonlanni"` or `"gaussian"` |
| | `NA`, `wavelength`, `M`, `ns`, `ng`, `ni`, `ti0`, `tg0`, `res_lateral`, `res_axial` | Gibson–Lanni optical parameters — must match your microscope |
| | `mid` | Hidden channels in NeurIPSF MLP |
| | `poisson_weight`, `sig_eps` | Initial noise model values (both learnable during fine-tuning) |
| | `device` | `"cuda:0"` or `"cpu"` |
| `pretrain_dataset` | `folderpath` | Path to synthetic volumes generated by `data_generation/` |
| | `size` | Full volume dimensions `[Z, Y, X]` |
| | `cropsize` | Patch dimensions fed to the model `[Z, Y, X]` |
| | `I` | Patches sampled per epoch |
| | `low` / `high` | Brightness index range for patch acceptance sampling |
| `train_dataset` / `val_dataset` | `folderpath` | Path to real TPM volumes |
| | `train` | `true` for train split, `false` for val split |
| | `mask`, `mask_num`, `mask_size` | Random masking augmentation |
| `test_dataset` | `test_images` | List of image filename stems held out of the train/val split |
| `pretrain_loop` | `loss_fnx` / `loss_fnz` | Segmentation and brightness losses as eval strings (e.g. `"nn.BCELoss()"`) |
| | `is_vibrate` | Simulate z-stack misalignment during pre-training |
| | `ewc` | `null` to disable EWC during pre-training |
| `train_loop` | `ewc` | `true` to enable EWC during fine-tuning, `null` to disable |
| | `ewc_weight` | EWC loss weight λ_EWC |
| | `qloss_weight` | VQ loss weight |
| | `ploss_weight` | PSF regularisation loss weight |
| | `zloss_weight` | Luminance loss weight |
| | `mrfloss_weights` | MRF 26-neighbour weights `l_00`, `l_01`, `l_10`, `l_11` |
| `vibration` | `max_step`, `b_sigma`, `amp_alpha`, `omega_alpha`, `noise_inv_var_alpha` | Parameter ranges for z-stack misalignment motion model |
| `visualization` | `path`, `z_stack`, `x_slice`, `mip` | Output directory and slice indices for PNG export |

---

## Workflow

### Step 1 — Generate synthetic training data

```bash
python data_generation/makedata.py
```

Generates random 3D volumes (spheres, cubes, octahedra, lines with elastic deformation) and blurs them with the initialized PSF. Output goes to the folder specified in `simulation_data_generation.dataset_name`.

### Step 2 — Pre-train on synthetic data

```bash
cd training
python train_runner.py <model_name>
```

Reads `pretrain_dataset` and `pretrain_val_dataset` from the config. Trains the U-Net encoder to recover sharp structures from synthetically blurred volumes. Also pre-trains the NeurIPSF MLP against the Gibson–Lanni target.

Saves checkpoints to `model/<pretrained_model>.pt` and `model/<pretrained_model>_optim.pt`.

### Step 3a — Fine-tune on real TPM data

```bash
python training/finetuning.py <model_name> [options]
```

Loads the pre-trained checkpoint from `pretrained_model`, then jointly optimises the encoder and NeurIPSF on real TPM volumes via reconstruction consistency loss.

Uses `train_dataset`, `val_dataset`, `pretrain_dataset` (for EWC), and `train_loop` from the config.

| Argument | Default | Description |
|---|---|---|
| `model_name` | *(required)* | Config file stem in `experiments/configs/` |
| `-cv`, `--cross_validation` | `""` | Fold suffix appended to dataset `folderpath` values |
| `-t`, `--train_mode` | `old` | Parameters to optimise: `all`, `encoder`, `decoder`, or `old` |
| `--train_with_align` | off | Enable z-stack alignment during fine-tuning |
| `--just_wanna_see_loss` | off | Evaluate losses on the existing saved checkpoint without training |

```bash
# Standard fine-tuning
python training/finetuning.py JNet_beads

# Cross-validation fold 1
python training/finetuning.py JNet_beads

Saves to `model/<model_name>.pt`.

### Step 3b — Fine-tune with simulation data (alternative)

```bash
python training/finetuning_with_simulation.py <model_name> [--train_with_align]
```

Uses `pretrain_dataset` for both training and EWC (instead of real TPM data). Useful for the simulation experiment in Fig. 2 of the paper.

### Step 4 — Run inference

Inference is class-based. Use the appropriate class from `inference/inference.py` depending on your data type:

| Class | Use case |
|---|---|
| `BeadsInference` | Fluorescent bead images (quantitative volume evaluation) |
| `MicrogliaInference` | In vivo microglia TPM volumes |
| `SimulationInference` | Synthetic phantom validation |

Each class reads the same config file as training. Example usage in a notebook or script:

```python
import sys; sys.path.insert(0, '.')
from inference.inference import BeadsInference

infer = BeadsInference(
    model_name = "JNet_beads",
    pretrain   = False,       # True = use pretrained weights only; False = use fine-tuned
    threshold  = -1           # -1 = use threshold from config params
)

results = infer.get_result(datapath="_20231208_tsuji_beads_roi_stackreged")
metrics = infer.evaluate(results)
infer.visualize(results)
infer.psf_visualize()

print(f"Mean bead volume: {metrics['mean']:.3f} µm³ ± {metrics['sd']:.3f}")
```

```python
from inference.inference import MicrogliaInference

infer = MicrogliaInference(
    model_name    = "JNet_microglia",
    is_finetuning = True,
    with_align    = False   # True requires align_params in config
)
results = infer.get_result(num_results=10)
infer.visualize(results)
infer.psf_visualize()
```

All visualization output (MIPs, depth views, PSF before/after, heatmaps) is saved as PNG files to the path set in `visualization.path` in the config.

---

## Data & Weights

Training datasets for reproducing figure 3-5 manuscript figures:

**[https://zenodo.org/records/15545449](https://zenodo.org/records/15545449)**

All weights for 

Place downloaded model weights in `model/` and data folders in the root directory (matching the `folderpath` values in the config).

You can generate dataset for figure 2 by running `data_generateion/fig2a_data.ipynb` and/or `data_generateion/fig2d_data.ipynb`.

---

## Methods Summary

### Architecture

| Component | Details |
|---|---|
| Encoder (`JNet`) | 3D U-Net; channels `[16, 32, 64, 128, 256]`; ReLU; dropout 0.5; 2 residual blocks per level |
| Decoder | Differentiable forward model: FFT-based PSF convolution → downsampling → Gaussian noise |
| `NeurIPSF` | MLP: 2D coordinate input → 40 hidden units → PSF value; sigmoid activation; pre-trained with RProp against Gibson–Lanni target |
| Shape head | Sigmoid + VQ binarization layer (stop-gradient estimator) |
| Brightness head | Log-normal brightness estimation |

### Loss Functions

| Loss | Config key | Purpose |
|---|---|---|
| Reconstruction ℒ_rec | *(always on)* | Gaussian NLL between original and reconstructed blurred image |
| VQ ℒ_VQ | `qloss_weight` | Encourages discrete (binary) structural representations |
| MRF ℒ_MRF | `mrfloss_weights` | 26-neighbour spatial smoothness prior on segmentation |
| EWC ℒ_EWC | `ewc_weight` | Prevents catastrophic forgetting of pre-training |
| PSF ℒ_PSF | `ploss_weight` | Constrains learned PSF toward physically plausible shapes |
| Luminance ℒ_Lum | `zloss_weight` | Regularises brightness estimation against log-normal prior |

---

## Citation

```bibtex
@article{morita2025tenet,
  title   = {Unsupervised deep learning enables blur-free resolution-enhancement in two-photon microscopy},
  author  = {Morita, Haruhiko and Hayashi, Shuto and Tsuji, Takahiro and Kato, Daisuke and Wake, Hiroaki and Shimamura, Teppei},
  journal = {Cell Reports Methods},
  year    = {2025},
  note    = {Submitted}
}
```